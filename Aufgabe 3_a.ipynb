{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the solution to the Aufgabe 2_b_2, can we get the solution that, the third Autoencoder AE3 has larger Advantage compared with the other 2, becaused of more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Add a validation process to your training function6. After each training epoch,\n",
    "validate the model using the validation signals and print the validation loss\n",
    "averaged over the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "# Autoencoder Models\n",
    "\n",
    "\n",
    "class AE3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE3, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Linear(512, 384)\n",
    "        self.enc2 = nn.Linear(384, 256)\n",
    "        self.enc3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Linear(128, 256)\n",
    "        self.dec2 = nn.Linear(256, 384)\n",
    "        self.dec3 = nn.Linear(384, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.enc1(x))\n",
    "        x = F.tanh(self.enc2(x))\n",
    "        x = F.tanh(self.enc3(x))\n",
    "        x = F.tanh(self.dec1(x))\n",
    "        x = F.tanh(self.dec2(x))\n",
    "        x = F.tanh(self.dec3(x))\n",
    "        return x\n",
    "\n",
    "# Custom Audio Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, signal_length, frame_length, overlap):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files (list): List of paths to audio files.\n",
    "            signal_length (int): Desired length of the signal in samples (La).\n",
    "            frame_length (int): Frame length in samples (LF).\n",
    "            overlap (int): Overlap of frames in samples (O).\n",
    "        \"\"\"\n",
    "        self.audio_files = audio_files\n",
    "        self.signal_length = signal_length\n",
    "        self.frame_length = frame_length\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def _adjust_frame_length_for_testing(self, frame):\n",
    "        if frame.shape[1] < 512:\n",
    "            padding = 512 - frame.shape[1]\n",
    "            frame = F.pad(frame, (0, padding))\n",
    "        elif frame.shape[1] > 512:\n",
    "            frame = frame[:, :512]\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # Truncate or Zero-pad the signal\n",
    "        waveform = self._adjust_length(waveform)\n",
    "\n",
    "        # Normalize the signal\n",
    "        waveform = self._normalize(waveform)\n",
    "\n",
    "        # Segment into frames\n",
    "        frames = self._segment_into_frames(waveform)\n",
    "\n",
    "        # Adjust frame length for testing\n",
    "        adjusted_frames = torch.zeros((frames.shape[0], 512))\n",
    "        for i, frame in enumerate(frames):\n",
    "            adjusted_frames[i] = self._adjust_frame_length_for_testing(frame.unsqueeze(0))\n",
    "\n",
    "        return adjusted_frames\n",
    "        \n",
    "\n",
    "    def _adjust_length(self, waveform):\n",
    "        if waveform.shape[1] > self.signal_length:\n",
    "            return waveform[:, :self.signal_length]\n",
    "        elif waveform.shape[1] < self.signal_length:\n",
    "            padding = self.signal_length - waveform.shape[1]\n",
    "            return F.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            return waveform\n",
    "\n",
    "    def _normalize(self, waveform):\n",
    "        max_val = torch.max(torch.abs(waveform))\n",
    "        if max_val > 0:\n",
    "            return waveform / max_val\n",
    "        return waveform\n",
    "\n",
    "    def _segment_into_frames(self, waveform):\n",
    "        step = self.frame_length - self.overlap\n",
    "        num_frames = 1 + (waveform.shape[1] - self.frame_length) // step\n",
    "        frames = torch.zeros((num_frames, self.frame_length))\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            start = i * step\n",
    "            end = start + self.frame_length\n",
    "            frames[i] = waveform[0, start:end]\n",
    "\n",
    "        return frames\n",
    "\n",
    "# Training Function\n",
    "def train(model, train_loader, validation_loader, epochs, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            \n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "        avg_loss_train = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}, Average Train Loss: {avg_loss_train}\")\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data)\n",
    "                validation_loss += loss.item()\n",
    "        avg_validation_loss = validation_loss / len(validation_loader)\n",
    "        print(f\"Epoch {epoch}, Validation Loss: {avg_validation_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing Function\n",
    "def test(model, test_loader, device, overlap=0):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_snr = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            #print(data.shape)\n",
    "            reconstructed = model(data)\n",
    "            original_signal = overlap_add(data, overlap)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            snr = calculate_snr(original_signal, reconstructed_signal)\n",
    "            print(f\"Signal {count}, SNR: {snr}\")\n",
    "            total_snr += snr\n",
    "            count += 1\n",
    "\n",
    "    avg_snr = total_snr / count\n",
    "    print(f\"Average SNR: {avg_snr}\")\n",
    "\n",
    "# Helper Functions\n",
    "def calculate_snr(original, reconstructed):\n",
    "    noise = original - reconstructed\n",
    "    signal_power = torch.mean(original ** 2)\n",
    "    noise_power = torch.mean(noise ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    return snr.item()\n",
    "\n",
    "\n",
    "\n",
    "def overlap_add(frames, overlap):\n",
    "    frames = frames.squeeze()\n",
    "    frame_length = frames.shape[1] #L\n",
    "    frame_zahl= frames.shape[0] #N\n",
    "    step = frame_length - overlap\n",
    "    signal_length = step * (frames.shape[0] - 1) + frame_length\n",
    "    signal = torch.zeros(signal_length)\n",
    "    window = torch.hann_window(frame_length)  # Hann Rectangular window\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(frame_zahl):\n",
    "        start = i *step\n",
    "        end = start + frame_length\n",
    "        #print(end-start, frames[i].size(), window.size())\n",
    "        signal[start:end] += frames[i] * window\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and save one of the reconstructed test signal for each of the autoencoder\n",
    "def test_and_save(model, test_loader, device, overlap, model_name, sampling_rate):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process only the first batch from the test loader\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed = model(data)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            \n",
    "            # Save the reconstructed signal to a WAV file\n",
    "            filename = f\"{model_name}_reconstructed.wav\"\n",
    "            torchaudio.save(filename, reconstructed_signal.unsqueeze(0), sampling_rate)\n",
    "            print(f\"Reconstructed signal saved as {filename}\")\n",
    "\n",
    "            break  # Process only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing AE3\n",
      "Epoch 0, Batch 0, Loss: 0.013360762037336826\n",
      "Epoch 0, Batch 1, Loss: 0.014740238897502422\n",
      "Epoch 0, Average Train Loss: 0.014050500467419624\n",
      "Epoch 0, Validation Loss: 0.012575080152601004\n",
      "Epoch 1, Batch 0, Loss: 0.011483998037874699\n",
      "Epoch 1, Batch 1, Loss: 0.015338124707341194\n",
      "Epoch 1, Average Train Loss: 0.013411061372607946\n",
      "Epoch 1, Validation Loss: 0.012347400188446045\n",
      "Epoch 2, Batch 0, Loss: 0.01212095282971859\n",
      "Epoch 2, Batch 1, Loss: 0.010705851018428802\n",
      "Epoch 2, Average Train Loss: 0.011413401924073696\n",
      "Epoch 2, Validation Loss: 0.011353570967912674\n",
      "Epoch 3, Batch 0, Loss: 0.011435553431510925\n",
      "Epoch 3, Batch 1, Loss: 0.010787340812385082\n",
      "Epoch 3, Average Train Loss: 0.011111447121948004\n",
      "Epoch 3, Validation Loss: 0.01024607103317976\n",
      "Epoch 4, Batch 0, Loss: 0.01062150951474905\n",
      "Epoch 4, Batch 1, Loss: 0.010837488807737827\n",
      "Epoch 4, Average Train Loss: 0.010729499161243439\n",
      "Epoch 4, Validation Loss: 0.009912596549838781\n",
      "Epoch 5, Batch 0, Loss: 0.010237694717943668\n",
      "Epoch 5, Batch 1, Loss: 0.008953004144132137\n",
      "Epoch 5, Average Train Loss: 0.009595349431037903\n",
      "Epoch 5, Validation Loss: 0.009676370769739151\n",
      "Epoch 6, Batch 0, Loss: 0.009178170934319496\n",
      "Epoch 6, Batch 1, Loss: 0.009766379371285439\n",
      "Epoch 6, Average Train Loss: 0.009472275152802467\n",
      "Epoch 6, Validation Loss: 0.008996872697025537\n",
      "Epoch 7, Batch 0, Loss: 0.00881869439035654\n",
      "Epoch 7, Batch 1, Loss: 0.00811376329511404\n",
      "Epoch 7, Average Train Loss: 0.00846622884273529\n",
      "Epoch 7, Validation Loss: 0.00875383336097002\n",
      "Epoch 8, Batch 0, Loss: 0.008240122348070145\n",
      "Epoch 8, Batch 1, Loss: 0.007318131625652313\n",
      "Epoch 8, Average Train Loss: 0.007779126986861229\n",
      "Epoch 8, Validation Loss: 0.007711723446846008\n",
      "Epoch 9, Batch 0, Loss: 0.007482361048460007\n",
      "Epoch 9, Batch 1, Loss: 0.007571329828351736\n",
      "Epoch 9, Average Train Loss: 0.007526845438405871\n",
      "Epoch 9, Validation Loss: 0.007053809240460396\n",
      "Epoch 10, Batch 0, Loss: 0.0071636647917330265\n",
      "Epoch 10, Batch 1, Loss: 0.006460723467171192\n",
      "Epoch 10, Average Train Loss: 0.006812194129452109\n",
      "Epoch 10, Validation Loss: 0.0069498480297625065\n",
      "Epoch 11, Batch 0, Loss: 0.006412772461771965\n",
      "Epoch 11, Batch 1, Loss: 0.007265671622008085\n",
      "Epoch 11, Average Train Loss: 0.006839222041890025\n",
      "Epoch 11, Validation Loss: 0.00572932418435812\n",
      "Epoch 12, Batch 0, Loss: 0.006260117050260305\n",
      "Epoch 12, Batch 1, Loss: 0.005907073151320219\n",
      "Epoch 12, Average Train Loss: 0.006083595100790262\n",
      "Epoch 12, Validation Loss: 0.005935791879892349\n",
      "Epoch 13, Batch 0, Loss: 0.005890679080039263\n",
      "Epoch 13, Batch 1, Loss: 0.005716525949537754\n",
      "Epoch 13, Average Train Loss: 0.005803602514788508\n",
      "Epoch 13, Validation Loss: 0.005648744059726596\n",
      "Epoch 14, Batch 0, Loss: 0.005835299845784903\n",
      "Epoch 14, Batch 1, Loss: 0.004530588164925575\n",
      "Epoch 14, Average Train Loss: 0.005182944005355239\n",
      "Epoch 14, Validation Loss: 0.005456804763525724\n",
      "Epoch 15, Batch 0, Loss: 0.005264877341687679\n",
      "Epoch 15, Batch 1, Loss: 0.005421854089945555\n",
      "Epoch 15, Average Train Loss: 0.005343365715816617\n",
      "Epoch 15, Validation Loss: 0.005095150787383318\n",
      "Epoch 16, Batch 0, Loss: 0.005151520483195782\n",
      "Epoch 16, Batch 1, Loss: 0.00475120497867465\n",
      "Epoch 16, Average Train Loss: 0.004951362730935216\n",
      "Epoch 16, Validation Loss: 0.004546776646748185\n",
      "Epoch 17, Batch 0, Loss: 0.005053207278251648\n",
      "Epoch 17, Batch 1, Loss: 0.004093337804079056\n",
      "Epoch 17, Average Train Loss: 0.004573272541165352\n",
      "Epoch 17, Validation Loss: 0.004944999236613512\n",
      "Epoch 18, Batch 0, Loss: 0.004550828132778406\n",
      "Epoch 18, Batch 1, Loss: 0.005183527711778879\n",
      "Epoch 18, Average Train Loss: 0.004867177922278643\n",
      "Epoch 18, Validation Loss: 0.004636317957192659\n",
      "Epoch 19, Batch 0, Loss: 0.004672566894441843\n",
      "Epoch 19, Batch 1, Loss: 0.0038733663968741894\n",
      "Epoch 19, Average Train Loss: 0.004272966645658016\n",
      "Epoch 19, Validation Loss: 0.004195315879769623\n",
      "Signal 0, SNR: 2.989058494567871\n",
      "Signal 1, SNR: 1.5257538557052612\n",
      "Signal 2, SNR: 5.026003837585449\n",
      "Signal 3, SNR: 2.903402328491211\n",
      "Signal 4, SNR: 3.712216377258301\n",
      "Signal 5, SNR: 5.622682094573975\n",
      "Signal 6, SNR: 3.8339710235595703\n",
      "Signal 7, SNR: 2.5043771266937256\n",
      "Signal 8, SNR: 1.6762111186981201\n",
      "Signal 9, SNR: 6.781145095825195\n",
      "Signal 10, SNR: 6.195812225341797\n",
      "Signal 11, SNR: 6.3463263511657715\n",
      "Signal 12, SNR: 6.844949722290039\n",
      "Signal 13, SNR: 4.728390693664551\n",
      "Signal 14, SNR: 4.578585147857666\n",
      "Signal 15, SNR: 2.656503200531006\n",
      "Average SNR: 4.245336793363094\n",
      "Reconstructed signal saved as AE3_reconstructed.wav\n"
     ]
    }
   ],
   "source": [
    "# Assuming a sampling rate of 16 kHz\n",
    "sampling_rate = 16000\n",
    "signal_length_seconds = 6  # 6 seconds\n",
    "signal_length_samples = sampling_rate * signal_length_seconds  # Convert to samples\n",
    "\n",
    "# Function to get audio file paths\n",
    "def get_audio_files(directory_path):\n",
    "    return [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.wav')]\n",
    "\n",
    "# Directory path for training data\n",
    "train_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Train_40'\n",
    "train_audio_files = get_audio_files(train_directory_path)\n",
    "\n",
    "# Directory path for validation data\n",
    "validation_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Dev_16'\n",
    "validation_audio_files = get_audio_files(train_directory_path)\n",
    "\n",
    "\n",
    "# Directory path for test data\n",
    "test_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Test_16'\n",
    "test_audio_files = get_audio_files(test_directory_path)\n",
    "\n",
    "\n",
    "\n",
    "# Training and testing data loaders\n",
    "frame_length = 512\n",
    "frame_length_test = 512\n",
    "\n",
    "# Update the Overlap\n",
    "overlap = int(0.5 * frame_length)\n",
    "\n",
    "\n",
    "train_dataset = AudioDataset(train_audio_files, signal_length_samples, frame_length, overlap)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataset = AudioDataset(validation_audio_files, signal_length_samples, frame_length, overlap)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = AudioDataset(test_audio_files, signal_length_samples, frame_length_test, overlap)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to train and test a model\n",
    "def train_and_test_model(model_class, train_loader, validation_loader, test_loader, device, model_name):\n",
    "    model = model_class().to(device)\n",
    "    train(model, train_loader, validation_loader, 20, device)  \n",
    "    test(model, test_loader, device)\n",
    "    test_and_save(model, test_loader, device, 0, model_name,  sampling_rate)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Train and test AE3\n",
    "print(\"Training and Testing AE3\")\n",
    "train_and_test_model(AE3, train_loader, validation_loader, test_loader, device, \"AE3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
