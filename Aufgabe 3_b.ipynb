{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Add an ”early-stopping mechanism” to your training function. The ”earlystopping\n",
    "mechanism” stops the training if the validation loss is not improved\n",
    "after a desired number of consecutive epochs (also called ”patience”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "# Autoencoder Models\n",
    "\n",
    "\n",
    "class AE3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE3, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Linear(512, 384)\n",
    "        self.enc2 = nn.Linear(384, 256)\n",
    "        self.enc3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Linear(128, 256)\n",
    "        self.dec2 = nn.Linear(256, 384)\n",
    "        self.dec3 = nn.Linear(384, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.enc1(x))\n",
    "        x = F.tanh(self.enc2(x))\n",
    "        x = F.tanh(self.enc3(x))\n",
    "        x = F.tanh(self.dec1(x))\n",
    "        x = F.tanh(self.dec2(x))\n",
    "        x = F.tanh(self.dec3(x))\n",
    "        return x\n",
    "\n",
    "# Custom Audio Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, signal_length, frame_length, overlap):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files (list): List of paths to audio files.\n",
    "            signal_length (int): Desired length of the signal in samples (La).\n",
    "            frame_length (int): Frame length in samples (LF).\n",
    "            overlap (int): Overlap of frames in samples (O).\n",
    "        \"\"\"\n",
    "        self.audio_files = audio_files\n",
    "        self.signal_length = signal_length\n",
    "        self.frame_length = frame_length\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def _adjust_frame_length_for_testing(self, frame):\n",
    "        if frame.shape[1] < 512:\n",
    "            padding = 512 - frame.shape[1]\n",
    "            frame = F.pad(frame, (0, padding))\n",
    "        elif frame.shape[1] > 512:\n",
    "            frame = frame[:, :512]\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # Truncate or Zero-pad the signal\n",
    "        waveform = self._adjust_length(waveform)\n",
    "\n",
    "        # Normalize the signal\n",
    "        waveform = self._normalize(waveform)\n",
    "\n",
    "        # Segment into frames\n",
    "        frames = self._segment_into_frames(waveform)\n",
    "\n",
    "        # Adjust frame length for testing\n",
    "        adjusted_frames = torch.zeros((frames.shape[0], 512))\n",
    "        for i, frame in enumerate(frames):\n",
    "            adjusted_frames[i] = self._adjust_frame_length_for_testing(frame.unsqueeze(0))\n",
    "\n",
    "        return adjusted_frames\n",
    "        \n",
    "\n",
    "    def _adjust_length(self, waveform):\n",
    "        if waveform.shape[1] > self.signal_length:\n",
    "            return waveform[:, :self.signal_length]\n",
    "        elif waveform.shape[1] < self.signal_length:\n",
    "            padding = self.signal_length - waveform.shape[1]\n",
    "            return F.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            return waveform\n",
    "\n",
    "    def _normalize(self, waveform):\n",
    "        max_val = torch.max(torch.abs(waveform))\n",
    "        if max_val > 0:\n",
    "            return waveform / max_val\n",
    "        return waveform\n",
    "\n",
    "    def _segment_into_frames(self, waveform):\n",
    "        step = self.frame_length - self.overlap\n",
    "        num_frames = 1 + (waveform.shape[1] - self.frame_length) // step\n",
    "        frames = torch.zeros((num_frames, self.frame_length))\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            start = i * step\n",
    "            end = start + self.frame_length\n",
    "            frames[i] = waveform[0, start:end]\n",
    "\n",
    "        return frames\n",
    "\n",
    "# Training Function\n",
    "def train(model, train_loader, validation_loader, epochs, device, patience):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            \n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "        avg_loss_train = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}, Average Train Loss: {avg_loss_train}\")\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data)\n",
    "                validation_loss += loss.item()\n",
    "        avg_validation_loss = validation_loss / len(validation_loader)\n",
    "        print(f\"Epoch {epoch}, Validation Loss: {avg_validation_loss}\")\n",
    "        \n",
    "        #early stopping, checking if there are no improved loss\n",
    "        if avg_validation_loss < best_val_loss:\n",
    "            best_val_loss = avg_validation_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"continue training\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing Function\n",
    "def test(model, test_loader, device, overlap=0):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_snr = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            #print(data.shape)\n",
    "            reconstructed = model(data)\n",
    "            original_signal = overlap_add(data, overlap)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            snr = calculate_snr(original_signal, reconstructed_signal)\n",
    "            print(f\"Signal {count}, SNR: {snr}\")\n",
    "            total_snr += snr\n",
    "            count += 1\n",
    "\n",
    "    avg_snr = total_snr / count\n",
    "    print(f\"Average SNR: {avg_snr}\")\n",
    "\n",
    "# Helper Functions\n",
    "def calculate_snr(original, reconstructed):\n",
    "    noise = original - reconstructed\n",
    "    signal_power = torch.mean(original ** 2)\n",
    "    noise_power = torch.mean(noise ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    return snr.item()\n",
    "\n",
    "\n",
    "\n",
    "def overlap_add(frames, overlap):\n",
    "    frames = frames.squeeze()\n",
    "    frame_length = frames.shape[1] #L\n",
    "    frame_zahl= frames.shape[0] #N\n",
    "    step = frame_length - overlap\n",
    "    signal_length = step * (frames.shape[0] - 1) + frame_length\n",
    "    signal = torch.zeros(signal_length)\n",
    "    window = torch.hann_window(frame_length)  # Hann Rectangular window\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(frame_zahl):\n",
    "        start = i *step\n",
    "        end = start + frame_length\n",
    "        #print(end-start, frames[i].size(), window.size())\n",
    "        signal[start:end] += frames[i] * window\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and save one of the reconstructed test signal for each of the autoencoder\n",
    "def test_and_save(model, test_loader, device, overlap, model_name, sampling_rate):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process only the first batch from the test loader\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed = model(data)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            \n",
    "            # Save the reconstructed signal to a WAV file\n",
    "            filename = f\"{model_name}_reconstructed.wav\"\n",
    "            torchaudio.save(filename, reconstructed_signal.unsqueeze(0), sampling_rate)\n",
    "            print(f\"Reconstructed signal saved as {filename}\")\n",
    "\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing AE3\n",
      "Epoch 0, Batch 0, Loss: 0.01421335432678461\n",
      "Epoch 0, Batch 1, Loss: 0.011385312303900719\n",
      "Epoch 0, Average Train Loss: 0.012799333315342665\n",
      "Epoch 0, Validation Loss: 0.011346881277859211\n",
      "continue training\n",
      "Epoch 1, Batch 0, Loss: 0.012699340470135212\n",
      "Epoch 1, Batch 1, Loss: 0.010636617429554462\n",
      "Epoch 1, Average Train Loss: 0.011667978949844837\n",
      "Epoch 1, Validation Loss: 0.012148500420153141\n",
      "continue training\n",
      "Epoch 2, Batch 0, Loss: 0.01204271987080574\n",
      "Epoch 2, Batch 1, Loss: 0.011080863885581493\n",
      "Epoch 2, Average Train Loss: 0.011561791878193617\n",
      "Epoch 2, Validation Loss: 0.011759676970541477\n",
      "continue training\n",
      "Epoch 3, Batch 0, Loss: 0.011241165921092033\n",
      "Epoch 3, Batch 1, Loss: 0.011633866466581821\n",
      "Epoch 3, Average Train Loss: 0.011437516193836927\n",
      "Epoch 3, Validation Loss: 0.010838564950972795\n",
      "continue training\n",
      "Epoch 4, Batch 0, Loss: 0.010840644128620625\n",
      "Epoch 4, Batch 1, Loss: 0.009860705584287643\n",
      "Epoch 4, Average Train Loss: 0.010350674856454134\n",
      "Epoch 4, Validation Loss: 0.009886257816106081\n",
      "continue training\n",
      "Epoch 5, Batch 0, Loss: 0.009547295048832893\n",
      "Epoch 5, Batch 1, Loss: 0.01147365104407072\n",
      "Epoch 5, Average Train Loss: 0.010510473046451807\n",
      "Epoch 5, Validation Loss: 0.009332113899290562\n",
      "continue training\n",
      "Epoch 6, Batch 0, Loss: 0.00967261753976345\n",
      "Epoch 6, Batch 1, Loss: 0.0075974841602146626\n",
      "Epoch 6, Average Train Loss: 0.008635050849989057\n",
      "Epoch 6, Validation Loss: 0.008824821561574936\n",
      "continue training\n",
      "Epoch 7, Batch 0, Loss: 0.008689429610967636\n",
      "Epoch 7, Batch 1, Loss: 0.008152177557349205\n",
      "Epoch 7, Average Train Loss: 0.00842080358415842\n",
      "Epoch 7, Validation Loss: 0.007648918079212308\n",
      "continue training\n",
      "Epoch 8, Batch 0, Loss: 0.007822070270776749\n",
      "Epoch 8, Batch 1, Loss: 0.008657755330204964\n",
      "Epoch 8, Average Train Loss: 0.008239912800490856\n",
      "Epoch 8, Validation Loss: 0.006960610626265407\n",
      "continue training\n",
      "Epoch 9, Batch 0, Loss: 0.007414863910526037\n",
      "Epoch 9, Batch 1, Loss: 0.007722305133938789\n",
      "Epoch 9, Average Train Loss: 0.007568584522232413\n",
      "Epoch 9, Validation Loss: 0.006532896542921662\n",
      "continue training\n",
      "Epoch 10, Batch 0, Loss: 0.007182189263403416\n",
      "Epoch 10, Batch 1, Loss: 0.006312451791018248\n",
      "Epoch 10, Average Train Loss: 0.006747320527210832\n",
      "Epoch 10, Validation Loss: 0.006492225220426917\n",
      "continue training\n",
      "Epoch 11, Batch 0, Loss: 0.006403694860637188\n",
      "Epoch 11, Batch 1, Loss: 0.0072335293516516685\n",
      "Epoch 11, Average Train Loss: 0.006818612106144428\n",
      "Epoch 11, Validation Loss: 0.006052520824596286\n",
      "continue training\n",
      "Epoch 12, Batch 0, Loss: 0.006427380722016096\n",
      "Epoch 12, Batch 1, Loss: 0.005203144624829292\n",
      "Epoch 12, Average Train Loss: 0.005815262673422694\n",
      "Epoch 12, Validation Loss: 0.005837619304656982\n",
      "continue training\n",
      "Epoch 13, Batch 0, Loss: 0.005918883252888918\n",
      "Epoch 13, Batch 1, Loss: 0.0055280402302742004\n",
      "Epoch 13, Average Train Loss: 0.005723461741581559\n",
      "Epoch 13, Validation Loss: 0.005588816944509745\n",
      "continue training\n",
      "Epoch 14, Batch 0, Loss: 0.005626515485346317\n",
      "Epoch 14, Batch 1, Loss: 0.005173800978809595\n",
      "Epoch 14, Average Train Loss: 0.005400158232077956\n",
      "Epoch 14, Validation Loss: 0.005298528354614973\n",
      "continue training\n",
      "Epoch 15, Batch 0, Loss: 0.005385842639952898\n",
      "Epoch 15, Batch 1, Loss: 0.004827085882425308\n",
      "Epoch 15, Average Train Loss: 0.005106464261189103\n",
      "Epoch 15, Validation Loss: 0.004915273515507579\n",
      "continue training\n",
      "Epoch 16, Batch 0, Loss: 0.005151422694325447\n",
      "Epoch 16, Batch 1, Loss: 0.004604589194059372\n",
      "Epoch 16, Average Train Loss: 0.0048780059441924095\n",
      "Epoch 16, Validation Loss: 0.005219374084845185\n",
      "continue training\n",
      "Epoch 17, Batch 0, Loss: 0.004982446786016226\n",
      "Epoch 17, Batch 1, Loss: 0.004348135087639093\n",
      "Epoch 17, Average Train Loss: 0.00466529093682766\n",
      "Epoch 17, Validation Loss: 0.004257828579284251\n",
      "continue training\n",
      "Epoch 18, Batch 0, Loss: 0.004646852146834135\n",
      "Epoch 18, Batch 1, Loss: 0.004780554678291082\n",
      "Epoch 18, Average Train Loss: 0.004713703412562609\n",
      "Epoch 18, Validation Loss: 0.004467085935175419\n",
      "continue training\n",
      "Epoch 19, Batch 0, Loss: 0.004443005658686161\n",
      "Epoch 19, Batch 1, Loss: 0.004857345018535852\n",
      "Epoch 19, Average Train Loss: 0.004650175338611007\n",
      "Epoch 19, Validation Loss: 0.004474878776818514\n",
      "continue training\n",
      "Signal 0, SNR: 3.0289416313171387\n",
      "Signal 1, SNR: 1.5444691181182861\n",
      "Signal 2, SNR: 5.089966773986816\n",
      "Signal 3, SNR: 3.1617343425750732\n",
      "Signal 4, SNR: 3.60295033454895\n",
      "Signal 5, SNR: 5.589349746704102\n",
      "Signal 6, SNR: 3.8368537425994873\n",
      "Signal 7, SNR: 2.571418285369873\n",
      "Signal 8, SNR: 1.696933627128601\n",
      "Signal 9, SNR: 6.9974260330200195\n",
      "Signal 10, SNR: 6.269236087799072\n",
      "Signal 11, SNR: 6.4185662269592285\n",
      "Signal 12, SNR: 6.750149250030518\n",
      "Signal 13, SNR: 4.944487571716309\n",
      "Signal 14, SNR: 4.681419849395752\n",
      "Signal 15, SNR: 2.7154903411865234\n",
      "Average SNR: 4.306212060153484\n",
      "Reconstructed signal saved as AE3_reconstructed.wav\n"
     ]
    }
   ],
   "source": [
    "# Assuming a sampling rate of 16 kHz\n",
    "sampling_rate = 16000\n",
    "signal_length_seconds = 6  \n",
    "signal_length_samples = sampling_rate * signal_length_seconds  \n",
    "\n",
    "# Function to get audio file paths\n",
    "def get_audio_files(directory_path):\n",
    "    return [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.wav')]\n",
    "\n",
    "# Directory path for training data\n",
    "train_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Train_40'\n",
    "train_audio_files = get_audio_files(train_directory_path)\n",
    "\n",
    "# Directory path for validation data\n",
    "validation_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Dev_16'\n",
    "validation_audio_files = get_audio_files(train_directory_path)\n",
    "\n",
    "\n",
    "# Directory path for test data\n",
    "test_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Test_16'\n",
    "test_audio_files = get_audio_files(test_directory_path)\n",
    "\n",
    "\n",
    "\n",
    "# Training and testing data loaders\n",
    "frame_length = 512\n",
    "frame_length_test = 512\n",
    "\n",
    "# Update the Overlap\n",
    "overlap = int(0.5 * frame_length)\n",
    "\n",
    "\n",
    "train_dataset = AudioDataset(train_audio_files, signal_length_samples, frame_length, overlap)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataset = AudioDataset(validation_audio_files, signal_length_samples, frame_length, overlap)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = AudioDataset(test_audio_files, signal_length_samples, frame_length_test, overlap)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to train and test a model\n",
    "def train_and_test_model(model_class, train_loader, validation_loader, test_loader, device, model_name, patience):\n",
    "    model = model_class().to(device)\n",
    "    train(model, train_loader, validation_loader, 20, device, patience)  \n",
    "    test(model, test_loader, device)\n",
    "    test_and_save(model, test_loader, device, 0, model_name,  sampling_rate)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "patience = 5  # Number of epochs before stopping\n",
    "\n",
    "# Train and test AE3\n",
    "print(\"Training and Testing AE3\")\n",
    "train_and_test_model(AE3, train_loader, validation_loader, test_loader, device, \"AE3\", patience)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
