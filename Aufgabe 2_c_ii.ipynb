{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Use an overlap that equals O = LF âˆ’ 1 and the Hann window for the\n",
    "overlap-add reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "# Autoencoder Models\n",
    "\n",
    "\n",
    "class AE3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE3, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Linear(512, 384)\n",
    "        self.enc2 = nn.Linear(384, 256)\n",
    "        self.enc3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Linear(128, 256)\n",
    "        self.dec2 = nn.Linear(256, 384)\n",
    "        self.dec3 = nn.Linear(384, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.enc1(x))\n",
    "        x = F.tanh(self.enc2(x))\n",
    "        x = F.tanh(self.enc3(x))\n",
    "        x = F.tanh(self.dec1(x))\n",
    "        x = F.tanh(self.dec2(x))\n",
    "        x = F.tanh(self.dec3(x))\n",
    "        return x\n",
    "\n",
    "# Custom Audio Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, signal_length, frame_length, overlap):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files (list): List of paths to audio files.\n",
    "            signal_length (int): Desired length of the signal in samples (La).\n",
    "            frame_length (int): Frame length in samples (LF).\n",
    "            overlap (int): Overlap of frames in samples (O).\n",
    "        \"\"\"\n",
    "        self.audio_files = audio_files\n",
    "        self.signal_length = signal_length\n",
    "        self.frame_length = frame_length\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def _adjust_frame_length_for_testing(self, frame):\n",
    "        if frame.shape[1] < 512:\n",
    "            padding = 512 - frame.shape[1]\n",
    "            frame = F.pad(frame, (0, padding))\n",
    "        elif frame.shape[1] > 512:\n",
    "            frame = frame[:, :512]\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # Truncate or Zero-pad the signal\n",
    "        waveform = self._adjust_length(waveform)\n",
    "\n",
    "        # Normalize the signal\n",
    "        waveform = self._normalize(waveform)\n",
    "\n",
    "        # Segment into frames\n",
    "        frames = self._segment_into_frames(waveform)\n",
    "\n",
    "        # Adjust frame length for testing\n",
    "        adjusted_frames = torch.zeros((frames.shape[0], 512))\n",
    "        for i, frame in enumerate(frames):\n",
    "            adjusted_frames[i] = self._adjust_frame_length_for_testing(frame.unsqueeze(0))\n",
    "\n",
    "        return adjusted_frames\n",
    "        \n",
    "\n",
    "    def _adjust_length(self, waveform):\n",
    "        if waveform.shape[1] > self.signal_length:\n",
    "            return waveform[:, :self.signal_length]\n",
    "        elif waveform.shape[1] < self.signal_length:\n",
    "            padding = self.signal_length - waveform.shape[1]\n",
    "            return F.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            return waveform\n",
    "\n",
    "    def _normalize(self, waveform):\n",
    "        max_val = torch.max(torch.abs(waveform))\n",
    "        if max_val > 0:\n",
    "            return waveform / max_val\n",
    "        return waveform\n",
    "\n",
    "    def _segment_into_frames(self, waveform):\n",
    "        step = self.frame_length - self.overlap\n",
    "        num_frames = 1 + (waveform.shape[1] - self.frame_length) // step\n",
    "        frames = torch.zeros((num_frames, self.frame_length))\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            start = i * step\n",
    "            end = start + self.frame_length\n",
    "            frames[i] = waveform[0, start:end]\n",
    "\n",
    "        return frames\n",
    "\n",
    "# Training Function\n",
    "def train(model, train_loader, epochs, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss}\")\n",
    "\n",
    "# Testing Function\n",
    "def test(model, test_loader, device, overlap=0):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_snr = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            #print(data.shape)\n",
    "            reconstructed = model(data)\n",
    "            original_signal = overlap_add(data, overlap)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            snr = calculate_snr(original_signal, reconstructed_signal)\n",
    "            print(f\"Signal {count}, SNR: {snr}\")\n",
    "            total_snr += snr\n",
    "            count += 1\n",
    "\n",
    "    avg_snr = total_snr / count\n",
    "    print(f\"Average SNR: {avg_snr}\")\n",
    "\n",
    "# Helper Functions\n",
    "def calculate_snr(original, reconstructed):\n",
    "    noise = original - reconstructed\n",
    "    signal_power = torch.mean(original ** 2)\n",
    "    noise_power = torch.mean(noise ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    return snr.item()\n",
    "\n",
    "\n",
    "\n",
    "def overlap_add(frames, overlap):\n",
    "    frames = frames.squeeze()\n",
    "    frame_length = frames.shape[1] #L\n",
    "    frame_zahl= frames.shape[0] #N\n",
    "    step = frame_length - overlap\n",
    "    signal_length = step * (frames.shape[0] - 1) + frame_length\n",
    "    signal = torch.zeros(signal_length)\n",
    "    window = torch.hann_window(frame_length)  # Hann Rectangular window\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(frame_zahl):\n",
    "        start = i *step\n",
    "        end = start + frame_length\n",
    "        #print(end-start, frames[i].size(), window.size())\n",
    "        signal[start:end] += frames[i] * window\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and save one of the reconstructed test signal for each of the autoencoder\n",
    "def test_and_save(model, test_loader, device, overlap, model_name, sampling_rate):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process only the first batch from the test loader\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed = model(data)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            \n",
    "            # Save the reconstructed signal to a WAV file\n",
    "            filename = f\"{model_name}_reconstructed.wav\"\n",
    "            torchaudio.save(filename, reconstructed_signal.unsqueeze(0), sampling_rate)\n",
    "            print(f\"Reconstructed signal saved as {filename}\")\n",
    "\n",
    "            break  # Process only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing AE3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Train and test AE3\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining and Testing AE3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtrain_and_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAE3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAE3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m, in \u001b[0;36mtrain_and_test_model\u001b[1;34m(model_class, train_loader, test_loader, device, model_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_test_model\u001b[39m(model_class, train_loader, test_loader, device, model_name):\n\u001b[0;32m     40\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Corrected the order of arguments\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     test(model, test_loader, device)\n\u001b[0;32m     43\u001b[0m     test_and_save(model, test_loader, device, \u001b[38;5;241m0\u001b[39m, model_name,  sampling_rate)\n",
      "Cell \u001b[1;32mIn[1], line 117\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, epochs, device)\u001b[0m\n\u001b[0;32m    114\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    115\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    118\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    119\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\yaolc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\yaolc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\yaolc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\yaolc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     67\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(waveform)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Segment into frames\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_segment_into_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Adjust frame length for testing\u001b[39;00m\n\u001b[0;32m     73\u001b[0m adjusted_frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m512\u001b[39m))\n",
      "Cell \u001b[1;32mIn[1], line 103\u001b[0m, in \u001b[0;36mAudioDataset._segment_into_frames\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    101\u001b[0m     start \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m step\n\u001b[0;32m    102\u001b[0m     end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_length\n\u001b[1;32m--> 103\u001b[0m     frames[i] \u001b[38;5;241m=\u001b[39m waveform[\u001b[38;5;241m0\u001b[39m, start:end]\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frames\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming a sampling rate of 16 kHz\n",
    "sampling_rate = 16000\n",
    "signal_length_seconds = 6  # 6 seconds\n",
    "signal_length_samples = sampling_rate * signal_length_seconds  # Convert to samples\n",
    "\n",
    "# Function to get audio file paths\n",
    "def get_audio_files(directory_path):\n",
    "    return [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.wav')]\n",
    "\n",
    "# Directory path for training data\n",
    "train_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\ComputerÃ¼bung 2\\signals\\Train_40'\n",
    "train_audio_files = get_audio_files(train_directory_path)\n",
    "\n",
    "# Directory path for test data\n",
    "test_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\ComputerÃ¼bung 2\\signals\\Test_16'\n",
    "test_audio_files = get_audio_files(test_directory_path)\n",
    "\n",
    "\n",
    "\n",
    "# Training and testing data loaders\n",
    "frame_length = 512\n",
    "frame_length_test = 512\n",
    "\n",
    "# Update the Overlap\n",
    "overlap = int(frame_length-1)\n",
    "\n",
    "\n",
    "train_dataset = AudioDataset(train_audio_files, signal_length_samples, frame_length, overlap)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = AudioDataset(test_audio_files, signal_length_samples, frame_length_test, overlap)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to train and test a model\n",
    "def train_and_test_model(model_class, train_loader, test_loader, device, model_name):\n",
    "    model = model_class().to(device)\n",
    "    train(model, train_loader, 20, device)  # Corrected the order of arguments\n",
    "    test(model, test_loader, device)\n",
    "    test_and_save(model, test_loader, device, 0, model_name,  sampling_rate)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Train and test AE3\n",
    "print(\"Training and Testing AE3\")\n",
    "train_and_test_model(AE3, train_loader, test_loader, device, \"AE3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
