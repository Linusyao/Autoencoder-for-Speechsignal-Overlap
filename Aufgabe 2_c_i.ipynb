{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the solution to the Aufgabe 2_b_2, can we get the solution that, the third Autoencoder AE3 has larger Advantage compared with the other 2, becaused of more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now choose the best autoencoder from task 2b) in terms of the individual\n",
    "losses and compare different overlap values using the 16 test signals again.\n",
    "i. Use an overlap O that equals O = 0.5 LF and the Hann window for the\n",
    "overlap-add reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "# Autoencoder Models\n",
    "\n",
    "\n",
    "class AE3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE3, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Linear(512, 384)\n",
    "        self.enc2 = nn.Linear(384, 256)\n",
    "        self.enc3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Linear(128, 256)\n",
    "        self.dec2 = nn.Linear(256, 384)\n",
    "        self.dec3 = nn.Linear(384, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.enc1(x))\n",
    "        x = F.tanh(self.enc2(x))\n",
    "        x = F.tanh(self.enc3(x))\n",
    "        x = F.tanh(self.dec1(x))\n",
    "        x = F.tanh(self.dec2(x))\n",
    "        x = F.tanh(self.dec3(x))\n",
    "        return x\n",
    "\n",
    "# Custom Audio Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, signal_length, frame_length, overlap):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files (list): List of paths to audio files.\n",
    "            signal_length (int): Desired length of the signal in samples (La).\n",
    "            frame_length (int): Frame length in samples (LF).\n",
    "            overlap (int): Overlap of frames in samples (O).\n",
    "        \"\"\"\n",
    "        self.audio_files = audio_files\n",
    "        self.signal_length = signal_length\n",
    "        self.frame_length = frame_length\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def _adjust_frame_length_for_testing(self, frame):\n",
    "        if frame.shape[1] < 512:\n",
    "            padding = 512 - frame.shape[1]\n",
    "            frame = F.pad(frame, (0, padding))\n",
    "        elif frame.shape[1] > 512:\n",
    "            frame = frame[:, :512]\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # Truncate or Zero-pad the signal\n",
    "        waveform = self._adjust_length(waveform)\n",
    "\n",
    "        # Normalize the signal\n",
    "        waveform = self._normalize(waveform)\n",
    "\n",
    "        # Segment into frames\n",
    "        frames = self._segment_into_frames(waveform)\n",
    "\n",
    "        # Adjust frame length for testing\n",
    "        adjusted_frames = torch.zeros((frames.shape[0], 512))\n",
    "        for i, frame in enumerate(frames):\n",
    "            adjusted_frames[i] = self._adjust_frame_length_for_testing(frame.unsqueeze(0))\n",
    "\n",
    "        return adjusted_frames\n",
    "        \n",
    "\n",
    "    def _adjust_length(self, waveform):\n",
    "        if waveform.shape[1] > self.signal_length:\n",
    "            return waveform[:, :self.signal_length]\n",
    "        elif waveform.shape[1] < self.signal_length:\n",
    "            padding = self.signal_length - waveform.shape[1]\n",
    "            return F.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            return waveform\n",
    "\n",
    "    def _normalize(self, waveform):\n",
    "        max_val = torch.max(torch.abs(waveform))\n",
    "        if max_val > 0:\n",
    "            return waveform / max_val\n",
    "        return waveform\n",
    "\n",
    "    def _segment_into_frames(self, waveform):\n",
    "        step = self.frame_length - self.overlap\n",
    "        num_frames = 1 + (waveform.shape[1] - self.frame_length) // step\n",
    "        frames = torch.zeros((num_frames, self.frame_length))\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            start = i * step\n",
    "            end = start + self.frame_length\n",
    "            frames[i] = waveform[0, start:end]\n",
    "\n",
    "        return frames\n",
    "\n",
    "# Training Function\n",
    "def train(model, train_loader, epochs, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss}\")\n",
    "\n",
    "# Testing Function\n",
    "def test(model, test_loader, device, overlap=0):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_snr = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            #print(data.shape)\n",
    "            reconstructed = model(data)\n",
    "            original_signal = overlap_add(data, overlap)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            snr = calculate_snr(original_signal, reconstructed_signal)\n",
    "            print(f\"Signal {count}, SNR: {snr}\")\n",
    "            total_snr += snr\n",
    "            count += 1\n",
    "\n",
    "    avg_snr = total_snr / count\n",
    "    print(f\"Average SNR: {avg_snr}\")\n",
    "\n",
    "# Helper Functions\n",
    "def calculate_snr(original, reconstructed):\n",
    "    noise = original - reconstructed\n",
    "    signal_power = torch.mean(original ** 2)\n",
    "    noise_power = torch.mean(noise ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    return snr.item()\n",
    "\n",
    "\n",
    "\n",
    "def overlap_add(frames, overlap):\n",
    "    frames = frames.squeeze()\n",
    "    frame_length = frames.shape[1] #L\n",
    "    frame_zahl= frames.shape[0] #N\n",
    "    step = frame_length - overlap\n",
    "    signal_length = step * (frames.shape[0] - 1) + frame_length\n",
    "    signal = torch.zeros(signal_length)\n",
    "    window = torch.hann_window(frame_length)  # Hann Rectangular window\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(frame_zahl):\n",
    "        start = i *step\n",
    "        end = start + frame_length\n",
    "        #print(end-start, frames[i].size(), window.size())\n",
    "        signal[start:end] += frames[i] * window\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and save one of the reconstructed test signal for each of the autoencoder\n",
    "def test_and_save(model, test_loader, device, overlap, model_name, sampling_rate):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process only the first batch from the test loader\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed = model(data)\n",
    "            reconstructed_signal = overlap_add(reconstructed, overlap)\n",
    "            \n",
    "            # Save the reconstructed signal to a WAV file\n",
    "            filename = f\"{model_name}_reconstructed.wav\"\n",
    "            torchaudio.save(filename, reconstructed_signal.unsqueeze(0), sampling_rate)\n",
    "            print(f\"Reconstructed signal saved as {filename}\")\n",
    "\n",
    "            break  # Process only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing AE3\n",
      "Epoch 0, Batch 0, Loss: 0.013184979557991028\n",
      "Epoch 0, Batch 1, Loss: 0.015419304370880127\n",
      "Epoch 0, Average Loss: 0.014302141964435577\n",
      "Epoch 1, Batch 0, Loss: 0.012059497646987438\n",
      "Epoch 1, Batch 1, Loss: 0.013028234243392944\n",
      "Epoch 1, Average Loss: 0.012543865945190191\n",
      "Epoch 2, Batch 0, Loss: 0.011859286576509476\n",
      "Epoch 2, Batch 1, Loss: 0.011971045285463333\n",
      "Epoch 2, Average Loss: 0.011915165930986404\n",
      "Epoch 3, Batch 0, Loss: 0.011459112167358398\n",
      "Epoch 3, Batch 1, Loss: 0.01118173822760582\n",
      "Epoch 3, Average Loss: 0.011320425197482109\n",
      "Epoch 4, Batch 0, Loss: 0.01044656615704298\n",
      "Epoch 4, Batch 1, Loss: 0.011746084317564964\n",
      "Epoch 4, Average Loss: 0.011096325237303972\n",
      "Epoch 5, Batch 0, Loss: 0.009762215428054333\n",
      "Epoch 5, Batch 1, Loss: 0.01057964563369751\n",
      "Epoch 5, Average Loss: 0.010170930530875921\n",
      "Epoch 6, Batch 0, Loss: 0.009332768619060516\n",
      "Epoch 6, Batch 1, Loss: 0.008571162819862366\n",
      "Epoch 6, Average Loss: 0.008951965719461441\n",
      "Epoch 7, Batch 0, Loss: 0.008129517547786236\n",
      "Epoch 7, Batch 1, Loss: 0.009918858297169209\n",
      "Epoch 7, Average Loss: 0.009024187922477722\n",
      "Epoch 8, Batch 0, Loss: 0.008193960413336754\n",
      "Epoch 8, Batch 1, Loss: 0.006690422538667917\n",
      "Epoch 8, Average Loss: 0.0074421914760023355\n",
      "Epoch 9, Batch 0, Loss: 0.007485469803214073\n",
      "Epoch 9, Batch 1, Loss: 0.006826777011156082\n",
      "Epoch 9, Average Loss: 0.007156123407185078\n",
      "Epoch 10, Batch 0, Loss: 0.007352609187364578\n",
      "Epoch 10, Batch 1, Loss: 0.005024216137826443\n",
      "Epoch 10, Average Loss: 0.0061884126625955105\n",
      "Epoch 11, Batch 0, Loss: 0.006583509501069784\n",
      "Epoch 11, Batch 1, Loss: 0.0061265709809958935\n",
      "Epoch 11, Average Loss: 0.006355040241032839\n",
      "Epoch 12, Batch 0, Loss: 0.006204439327120781\n",
      "Epoch 12, Batch 1, Loss: 0.0059595354832708836\n",
      "Epoch 12, Average Loss: 0.006081987405195832\n",
      "Epoch 13, Batch 0, Loss: 0.006168120075017214\n",
      "Epoch 13, Batch 1, Loss: 0.004578486550599337\n",
      "Epoch 13, Average Loss: 0.005373303312808275\n",
      "Epoch 14, Batch 0, Loss: 0.005809789057821035\n",
      "Epoch 14, Batch 1, Loss: 0.004507780075073242\n",
      "Epoch 14, Average Loss: 0.005158784566447139\n",
      "Epoch 15, Batch 0, Loss: 0.005253707058727741\n",
      "Epoch 15, Batch 1, Loss: 0.00543485302478075\n",
      "Epoch 15, Average Loss: 0.005344280041754246\n",
      "Epoch 16, Batch 0, Loss: 0.005122973117977381\n",
      "Epoch 16, Batch 1, Loss: 0.004784994293004274\n",
      "Epoch 16, Average Loss: 0.0049539837054908276\n",
      "Epoch 17, Batch 0, Loss: 0.004969828762114048\n",
      "Epoch 17, Batch 1, Loss: 0.00432615214958787\n",
      "Epoch 17, Average Loss: 0.004647990455850959\n",
      "Epoch 18, Batch 0, Loss: 0.004478579852730036\n",
      "Epoch 18, Batch 1, Loss: 0.0053499317727983\n",
      "Epoch 18, Average Loss: 0.004914255812764168\n",
      "Epoch 19, Batch 0, Loss: 0.004449986387044191\n",
      "Epoch 19, Batch 1, Loss: 0.004611089359968901\n",
      "Epoch 19, Average Loss: 0.004530537873506546\n",
      "Signal 0, SNR: 2.9955837726593018\n",
      "Signal 1, SNR: 1.4941182136535645\n",
      "Signal 2, SNR: 5.013126373291016\n",
      "Signal 3, SNR: 3.3398008346557617\n",
      "Signal 4, SNR: 3.505316972732544\n",
      "Signal 5, SNR: 5.583858966827393\n",
      "Signal 6, SNR: 3.820511817932129\n",
      "Signal 7, SNR: 2.4969382286071777\n",
      "Signal 8, SNR: 1.6918439865112305\n",
      "Signal 9, SNR: 6.904792308807373\n",
      "Signal 10, SNR: 6.194039344787598\n",
      "Signal 11, SNR: 6.392864227294922\n",
      "Signal 12, SNR: 6.727832794189453\n",
      "Signal 13, SNR: 4.809861660003662\n",
      "Signal 14, SNR: 4.732740879058838\n",
      "Signal 15, SNR: 2.6411213874816895\n",
      "Average SNR: 4.271521985530853\n",
      "Reconstructed signal saved as AE3_reconstructed.wav\n"
     ]
    }
   ],
   "source": [
    "# Assuming a sampling rate of 16 kHz\n",
    "sampling_rate = 16000\n",
    "signal_length_seconds = 6  # 6 seconds\n",
    "signal_length_samples = sampling_rate * signal_length_seconds  # Convert to samples\n",
    "\n",
    "# Function to get audio file paths\n",
    "def get_audio_files(directory_path):\n",
    "    return [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.wav')]\n",
    "\n",
    "# Directory path for training data\n",
    "train_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Train_40'\n",
    "train_audio_files = get_audio_files(train_directory_path)\n",
    "\n",
    "# Directory path for test data\n",
    "test_directory_path = r'C:\\Kursmaterial\\Dl der Sprachsignalverarbeitung\\Computerübung 2\\signals\\Test_16'\n",
    "test_audio_files = get_audio_files(test_directory_path)\n",
    "\n",
    "\n",
    "\n",
    "# Training and testing data loaders\n",
    "frame_length = 512\n",
    "frame_length_test = 512\n",
    "\n",
    "# Update the Overlap\n",
    "overlap = int(0.5 * frame_length)\n",
    "\n",
    "\n",
    "train_dataset = AudioDataset(train_audio_files, signal_length_samples, frame_length, overlap)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = AudioDataset(test_audio_files, signal_length_samples, frame_length_test, overlap)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to train and test a model\n",
    "def train_and_test_model(model_class, train_loader, test_loader, device, model_name):\n",
    "    model = model_class().to(device)\n",
    "    train(model, train_loader, 20, device)  # Corrected the order of arguments\n",
    "    test(model, test_loader, device)\n",
    "    test_and_save(model, test_loader, device, 0, model_name,  sampling_rate)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Train and test AE3\n",
    "print(\"Training and Testing AE3\")\n",
    "train_and_test_model(AE3, train_loader, test_loader, device, \"AE3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
